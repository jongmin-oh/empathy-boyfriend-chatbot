import time
import random
from pathlib import Path

import streamlit as st
import torch
import numpy as np

from model import run, EOT_TOKEN

BASE_DIR = Path(__file__).resolve().parent

st.set_page_config(page_title="ìœ„ë¡œí•˜ëŠ” ë‚¨ì¹œ ì±—ë´‡", page_icon="ğŸ‘¦ğŸ»")
st.header("ìœ„ë¡œí•˜ëŠ” ë‚¨ì¹œ ì±—ë´‡", anchor="top", divider="rainbow")

# st.image(str(BASE_DIR.joinpath("assets", "boyfriend.jpeg")), width=200)


def seed_everything(seed):
    torch.manual_seed(seed)  # torchë¥¼ ê±°ì¹˜ëŠ” ëª¨ë“  ë‚œìˆ˜ë“¤ì˜ ìƒì„±ìˆœì„œë¥¼ ê³ ì •í•œë‹¤
    torch.cuda.manual_seed(seed)  # cudaë¥¼ ì‚¬ìš©í•˜ëŠ” ë©”ì†Œë“œë“¤ì˜ ë‚œìˆ˜ì‹œë“œëŠ” ë”°ë¡œ ê³ ì •í•´ì¤˜ì•¼í•œë‹¤
    torch.cuda.manual_seed_all(seed)  # if use multi-GPU
    torch.backends.cudnn.deterministic = True  # ë”¥ëŸ¬ë‹ì— íŠ¹í™”ëœ CuDNNì˜ ë‚œìˆ˜ì‹œë“œë„ ê³ ì •
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)  # numpyë¥¼ ì‚¬ìš©í•  ê²½ìš° ê³ ì •
    random.seed(seed)  # íŒŒì´ì¬ ìì²´ ëª¨ë“ˆ random ëª¨ë“ˆì˜ ì‹œë“œ ê³ ì •


if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if message["role"] != "system":
            st.markdown(message["content"])

if prompt := st.chat_input("ë‚¨ì¹œì—ê²Œ í•˜ê³ ì‹¶ì€ ë§ì„ ì…ë ¥í•˜ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        with st.spinner("ê³µê°ì¤‘...."):
            time.sleep(1)

        # seed_everything(42)
        stream = run(
            messages=st.session_state.messages,
            max_new_tokens=1024,
            temperature=0.3,
            top_p=0.95,
            top_k=50,
            repetition_penalty=1.2,
        )

        for response in stream:
            full_response = response.replace(EOT_TOKEN, "")
            message_placeholder.markdown(full_response + "â–Œ")

        message_placeholder.markdown(full_response)
    st.session_state.messages.append({"role": "assistant", "content": full_response})
